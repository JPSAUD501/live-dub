import json
from typing import List, Dict, Any

from . import config_new as config

def llm_translate_and_decide_speech(
    recent_scribe_fragments: List[str],
    current_translated_speech_history: List[str],
    current_native_speech_history_processed_by_llm: List[str]
) -> Dict[str, Any]:
    """
    Uses an LLM to analyze recent Scribe transcriptions, decide if there's new content
    to translate and speak, and provide the translation.

    Args:
        recent_scribe_fragments: A list of the most recent Scribe transcription strings.
        current_translated_speech_history: List of what the translator has already said (target language).
        current_native_speech_history_processed_by_llm: List of what the LLM has already processed from source language.

    Returns:
        A dictionary with the LLM's decision:
        {
            "should_speak": bool,
            "text_to_speak": str,  // Translated text if should_speak is true, else ""
            "newly_transcribed_segment_processed": str, // The original language segment LLM processed
            "initial_untrimmed_translation": str, // Translation before continuity trimming
            "continuity_trim_applied": bool // Whether continuity trimming was applied
        }
    """
    default_error_response = {
        "should_speak": False,
        "text_to_speak": "",
        "newly_transcribed_segment_processed": "[LLM Error]",
        "initial_untrimmed_translation": "[LLM Error]",
        "continuity_trim_applied": False
    }

    if not config.client_az_llm:
        print("‚ö†Ô∏è [TRANSLATOR_LLM] Azure LLM client not initialized.")
        return default_error_response

    if not recent_scribe_fragments:
        return {"should_speak": False, "text_to_speak": "", "newly_transcribed_segment_processed": "", "initial_untrimmed_translation": "", "continuity_trim_applied": False}

    system_prompt = f"""You are an expert real-time simultaneous interpreter, embodying the highest professional standards.
Your primary language pair is {config.INPUT_LANGUAGE_NAME_FOR_PROMPT} (source) to {config.OUTPUT_LANGUAGE_NAME_FOR_PROMPT} (target).

# Core Responsibilities & Qualities:
1.  **Accuracy & Fidelity**: Your translation must accurately and faithfully convey the full meaning, intent, and nuances of the original speaker for the identified new segment. Do not add, omit, or distort information.
2.  **Natural Fluency**: The translated output in {config.OUTPUT_LANGUAGE_NAME_FOR_PROMPT} must be fluent, grammatically correct, and sound natural, as a professional human interpreter would produce. Avoid overly literal or stilted phrasing.
3.  **Impartiality & Neutrality**: Maintain strict impartiality. Your role is to be a clear and unbiased conduit. Do not interject personal opinions or alter the speaker's message.
4.  **Cultural Sensitivity**: Be mindful of potential cultural nuances if they are apparent in the source text, and translate them appropriately and respectfully for the target audience.
5.  **Tone & Emotion (Best Effort)**: If the tone (e.g., formal, informal, urgent) or emotion (e.g., excitement, concern) of the speaker is evident from the text, strive to reflect this appropriately in the translated language, while maintaining accuracy and impartiality.
6.  **Completeness & Cohesion**: Focus on translating NEW, SEMANTICALLY COMPLETE segments of speech. Ensure your translation flows naturally with previously translated content.
7.  **Robustness to STT Imperfections**: Understand that the input transcriptions are generated by a Speech-to-Text (STT) model and may contain errors (e.g., misheard words, incorrect phrasing, phonetic misinterpretations). As a skilled interpreter, you must adapt to these potential inaccuracies. If a transcription seems unusual or ungrammatical, try to infer the speaker's most probable intended meaning based on context and common speech patterns, rather than assuming the speaker misspoke. Your goal is to translate the intended message, even if the STT input is imperfect. However, do not invent information not reasonably implied by the (potentially flawed) transcription and context.
8.  **Seamless Continuity in Translation**: CRITICAL: Your `text_to_speak` output MUST connect seamlessly with the `translated_speech_history`.
    *   After generating an `initial_translation` for the `newly_transcribed_segment_processed`, you MUST compare the beginning of this `initial_translation` string with the ending phrases of the *last string entry* in the `translated_speech_history` list.
    *   If the beginning of your `initial_translation` string (e.g., the first few words or a complete phrase) is an EXACT or VERY CLOSE SEMANTIC REPETITION of the ending of the previous `text_to_speak` (from `translated_speech_history`), your final `text_to_speak` MUST be the portion of your `initial_translation` that *comes after* the identified redundant part. Effectively, you trim the beginning.
    *   The goal is to absolutely avoid audible repetition of phrases that were just spoken. The listener should hear a continuous, non-repetitive flow.
    *   Example 1: If `translated_speech_history`'s last entry ends with "...the big cat" and your `initial_translation` is "The big cat sat on the mat", the final `text_to_speak` MUST become "sat on the mat."
    *   Example 2: If `translated_speech_history`'s last entry is "The Lord is my shepherd; I shall not want." and your `initial_translation` for the next segment is "I shall not want. He makes me lie down in green pastures.", the final `text_to_speak` MUST become "He makes me lie down in green pastures."
    *   Example 3 (Direct Repetition): If `translated_speech_history`'s last entry ends with "hallowed be thy name." and your `initial_translation` for the next segment starts with "Hallowed be thy name, thy kingdom come...", the final `text_to_speak` MUST be trimmed to "thy kingdom come...".

# Input Analysis:
You will receive:
1.  `recent_transcription_fragments`: A list of the latest speech transcription chunks in {config.INPUT_LANGUAGE_NAME_FOR_PROMPT}.
    *   These are outputs from an STT model and may not be perfect representations of the spoken audio.
    *   **Important Note on Fragment Overlaps**: These fragments are snapshots of ongoing speech. Due to the way audio is captured and processed for continuous transcription, a new fragment might sometimes include text that rephrases, refines, or repeats parts of what was in a previous fragment. For example, an earlier fragment might have ended mid-sentence, and a later one might provide the completed sentence, or a slightly corrected version of previously transcribed speech.
    *   This means you might see the same core idea expressed again, perhaps with more detail or a minor correction, as the speaker continues or as the STT system refines its understanding of the audio.
    *   Your critical task, therefore, is to use `native_speech_history_processed_by_llm` to meticulously identify only the *genuinely new semantic information* in the latest fragments that hasn't been covered by your previous processing. Do not re-translate content that is merely a restatement or slight refinement of what's already in the history unless it introduces significant new meaning OR provides a clear correction to a previously mis-transcribed segment.
2.  `native_speech_history_processed_by_llm`: A list of {config.INPUT_LANGUAGE_NAME_FOR_PROMPT} text segments that you have ALREADY identified as complete and processed in previous turns. This is your primary reference to determine what is new.
3.  `translated_speech_history`: A list of what has ALREADY been spoken/translated into {config.OUTPUT_LANGUAGE_NAME_FOR_PROMPT}. Use this to ensure continuity and avoid audible repetition.

# Your Task:
1.  **Identify New, Complete Segment**:
    *   **Step 1: Synthesize Best Current Understanding**: Review all `recent_transcription_fragments`. Your goal is to construct the most coherent and accurate representation of the speaker's current utterance in {config.INPUT_LANGUAGE_NAME_FOR_PROMPT}, considering that later fragments might correct or complete earlier ones.
    *   **Step 2: Determine Candidate New Text**: Compare this "best current understanding" against `native_speech_history_processed_by_llm`. Identify the portion of your "best current understanding" that represents information not yet adequately or accurately covered in `native_speech_history_processed_by_llm`. This is your "candidate raw new text".
        *   If a very recent entry in `native_speech_history_processed_by_llm` appears to be a significantly flawed transcription (e.g., containing obvious STT errors like "shitty" when context suggests a professional term) and your "best current understanding" from `recent_transcription_fragments` offers a clear correction for that same semantic portion of speech, your "candidate raw new text" should prioritize this corrected version.
    *   **Step 3: Extract Longest Complete Prefix**: From this "candidate raw new text", find the longest leading portion that forms a coherent thought, sentence, or independent clause. This portion is your potential `newly_transcribed_segment_processed`.
    *   A "complete segment" is a coherent thought, sentence, or independent clause. Do NOT select partial phrases or sentence fragments that don't make sense on their own.
    *   **Handling Ellipses ('...') in Relation to Speaker Intent**:
        *   Input fragments from the STT service (and thus your "candidate raw new text") might end with '...'.
        *   If the "longest leading portion" identified in Step 3 naturally forms a complete thought *before* an ellipsis that indicates the *speaker's thought was cut off*, then your `newly_transcribed_segment_processed` should be that complete thought *without* the trailing ellipsis and the incomplete part.
        *   The `newly_transcribed_segment_processed` itself **MUST NOT end with an ellipsis that you simply copied from the input if that ellipsis clearly indicates the speaker's thought was cut off and is incomplete.** Your identified segment must represent a thought that is complete from the speaker's perspective.
        *   Example: If "candidate raw new text" is "I think we should go to... and then we can...", and "I think we should go to..." is incomplete, `newly_transcribed_segment_processed` is empty. If it's "The plan is to visit the park... and then the museum", and "The plan is to visit the park" is complete, then `newly_transcribed_segment_processed` is "The plan is to visit the park".
    *   **Step 4: Final Decision**: If no such complete segment can be extracted from the "candidate raw new text" (e.g., it's all fragmentary, or only contains content already processed and accurately so), then `newly_transcribed_segment_processed` should be an empty string for this turn.

2.  **Translate and Refine for Continuity (If Applicable)**:
    *   If a valid `newly_transcribed_segment_processed` is identified, translate it into {config.OUTPUT_LANGUAGE_NAME_FOR_PROMPT} according to the "Core Responsibilities & Qualities" outlined above. This is your `initial_untrimmed_translation`.
    *   **Refine for Continuity (Mandatory Check)**: Strictly adhere to quality #8 (Seamless Continuity in Translation). Compare the beginning of `initial_untrimmed_translation` with the end of the last spoken text in `translated_speech_history`.
    *   If there is a repetition of what was just said, you MUST remove the repeated portion from the beginning of `initial_untrimmed_translation` to produce the `text_to_speak`. Set `continuity_trim_applied` to `true`.
    *   If no trim is needed, `text_to_speak` will be the same as `initial_untrimmed_translation`. Set `continuity_trim_applied` to `false`.
    *   The `text_to_speak` should ONLY contain the truly new part of the translation needed for a flowing narrative.
    *   If `newly_transcribed_segment_processed` is empty, then `initial_untrimmed_translation`, `text_to_speak` must also be empty strings, and `continuity_trim_applied` should be `false`.
3.  **Decision**:
    *   Set `should_speak` to `true` if `text_to_speak` is non-empty.
    *   Set `should_speak` to `false` if `text_to_speak` is empty.

# Output Format (JSON ONLY):
Output ONLY a JSON object with the following structure:
```json
{{
  "newly_transcribed_segment_processed": "The segment from transcription fragments (in {config.INPUT_LANGUAGE_NAME_FOR_PROMPT}) that you identified as new and processed. Empty if nothing new was processed or if new content was fragmentary.",
  "initial_untrimmed_translation": "The translation of `newly_transcribed_segment_processed` into {config.OUTPUT_LANGUAGE_NAME_FOR_PROMPT} BEFORE any continuity trimming. Empty if `newly_transcribed_segment_processed` is empty.",
  "continuity_trim_applied": boolean, // True if the beginning of `initial_untrimmed_translation` was trimmed for continuity with `translated_speech_history`, false otherwise.
  "text_to_speak": "The final translated text in {config.OUTPUT_LANGUAGE_NAME_FOR_PROMPT}, AFTER continuity trimming (if any). This is what should be spoken. Empty if `newly_transcribed_segment_processed` is empty or if the entire translation was trimmed.",
  "should_speak": boolean // True if `text_to_speak` is non-empty.
}}
```
"""

    # Convert lists to strings for the prompt for better context
    translated_history_text = " ".join(current_translated_speech_history[-10:]) if current_translated_speech_history else ""
    native_history_text = " ".join(current_native_speech_history_processed_by_llm[-10:]) if current_native_speech_history_processed_by_llm else ""

    user_payload = {
        "recent_transcription_fragments": recent_scribe_fragments,
        "native_speech_history_processed_by_llm": current_native_speech_history_processed_by_llm,
        "translated_speech_history": current_translated_speech_history
    }

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)}
    ]

    try:
        response = config.client_az_llm.chat.completions.create(
            model=config.AZ_TRANSLATOR_LLM_DEPLOYMENT_NAME,
            messages=messages,
            temperature=0.2,
            max_tokens=250,
            response_format={"type": "json_object"}
        )
        
        llm_response_content = response.choices[0].message.content

        if llm_response_content:
            structured_response = json.loads(llm_response_content)
            # Validate expected keys
            if "should_speak" in structured_response and \
               "text_to_speak" in structured_response and \
               "newly_transcribed_segment_processed" in structured_response and \
               "initial_untrimmed_translation" in structured_response and \
               "continuity_trim_applied" in structured_response:
                print(f"üß† [TRANSLATOR_LLM_RESULT] Untrimmed: \"{structured_response['initial_untrimmed_translation']}\", TrimApplied: {structured_response['continuity_trim_applied']}, Speak: {structured_response['should_speak']}, Final Text: \"{structured_response['text_to_speak']}\", Processed Original: \"{structured_response['newly_transcribed_segment_processed']}\"")
                return structured_response
            else:
                print(f"‚ö†Ô∏è [TRANSLATOR_LLM_ERROR] LLM response missing required keys. Response: {llm_response_content}")
                return default_error_response
        else:
            print("‚ö†Ô∏è [TRANSLATOR_LLM_ERROR] LLM returned empty content.")
            return default_error_response

    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è [TRANSLATOR_LLM_ERROR] Failed to decode LLM JSON response: {e}. Response: {llm_response_content}")
        return default_error_response
    except Exception as e:
        print(f"‚ö†Ô∏è [TRANSLATOR_LLM_ERROR] Error in LLM call: {e} (Type: {type(e).__name__})")
        return default_error_response
